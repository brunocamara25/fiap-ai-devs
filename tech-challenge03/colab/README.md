# ğŸ§  Fine-Tuning com LLaMA 3.2 3B para GeraÃ§Ã£o de DescriÃ§Ãµes de Produtos

## ğŸ“ DescriÃ§Ã£o Geral

Este repositÃ³rio Ã© parte do Tech Challenge e demonstra como realizar o fine-tuning de um modelo foundation (LLaMA 3.2 3B Instruct) usando o dataset "The AmazonTitles-1.3MM". O objetivo Ã© treinar o modelo para responder a perguntas sobre produtos, gerando suas descriÃ§Ãµes com base nos tÃ­tulos presentes no dataset.

## ğŸ“‚ Estrutura do Projeto

```
TechChallenge3/
â”œâ”€â”€ trn.json               # Dataset de treinamento
â”œâ”€â”€ tst.json               # Dataset de teste
â”œâ”€â”€ output/
â”‚   â””â”€â”€ train_prepared.jsonl / test_prepared.jsonl
â”œâ”€â”€ model/                 # DiretÃ³rio onde o modelo fine-tunado Ã© salvo
â””â”€â”€ notebook.ipynb         # Notebook com o treinamento e inferÃªncia
```

## ğŸ“Œ Objetivos

* Realizar o fine-tuning de um modelo LLaMA com base em perguntas sobre produtos
* Gerar descriÃ§Ãµes realistas e contextuais com base no tÃ­tulo
* Demonstrar o processo completo: preparaÃ§Ã£o, treinamento e inferÃªncia

## âš™ï¸ Requisitos

* Google Colab
* GPU (idealmente com suporte a 4-bit quantization)

## ğŸ§ª ExecuÃ§Ã£o

Abra o notebook `notebook.ipynb` no Google Colab e siga as etapas:

1. **Montar o Google Drive** (armazenamento de dados e modelo)
2. **Instalar dependÃªncias**
3. **Preparar os dados** (`title` â†’ prompt; `content` â†’ resposta)
4. **Carregar modelo** (`unsloth/Llama-3.2-3B-Instruct`)
5. **Executar fine-tuning** com LoRA e 4-bit
6. **Testar inferÃªncia** com exemplos reais
7. **Salvar modelo final** no diretÃ³rio `model/`

## ğŸ’¬ Exemplo de Uso

**Entrada**:

```text
Qual Ã© a descriÃ§Ã£o do produto com o tÃ­tulo: Girls Ballet Tutu Neon Pink?
```

**SaÃ­da gerada pelo modelo**:

```text
Este tutu de balÃ© rosa neon Ã© perfeito para meninas que adoram danÃ§ar. Feito com camadas de tule macio, proporciona conforto e estilo para qualquer apresentaÃ§Ã£o.
```

## ğŸ§  Modelo Utilizado

* `unsloth/Llama-3.2-3B-Instruct`
* Fine-tuning com LoRA
* QuantizaÃ§Ã£o em 4-bit

## ğŸ—ƒï¸ Dataset

* [The AmazonTitles-1.3MM](https://drive.google.com/file/d/12zH4mL2RX8iSvH0VCNnd3QxO4DzuHWnK/view)
* Campos utilizados: `title` (tÃ­tulo do produto) e `content` (descriÃ§Ã£o)

## ğŸ§¾ ParÃ¢metros de Treinamento

* batch: 2 (acumulado 4)
* max steps: 60
* learning rate: 2e-4
* warmup steps: 5
* weight decay: 0.01

## ğŸ¥ DemonstraÃ§Ã£o

Grave um vÃ­deo apresentando:

* Seu notebook
* Uma pergunta feita ao modelo
* A resposta gerada

## âœ… Resultado Final

O modelo consegue responder perguntas como:

> "Qual Ã© a descriÃ§Ã£o do produto com o tÃ­tulo: X?"

Utilizando os aprendizados do fine-tuning para gerar descriÃ§Ãµes coerentes com o conteÃºdo de treinamento.

## ğŸ“¬ Contato

Para dÃºvidas ou sugestÃµes, entre em contato via GitHub ou email pessoal.
