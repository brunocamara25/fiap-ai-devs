{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4ef2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# 📦 Montar o Drive\n",
    "# ===============================\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab68974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# 📦 Instalar Dependências\n",
    "# ===============================\n",
    "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "!pip install --no-deps xformers \"trl<0.9.0\" peft accelerate bitsandbytes\n",
    "!pip install transformers datasets trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e635a8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"/content/drive/MyDrive/TechChallenge3/\"\n",
    "OUTPUT_PATH_DATASET = \"/content/drive/MyDrive/TechChallenge3/output/\"\n",
    "MODEL_PATH_DATASET = \"/content/drive/MyDrive/TechChallenge3/model/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6667992",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# 🔄 Preparar Dados de Treinamento\n",
    "# ===============================\n",
    "import json\n",
    "\n",
    "SYSTEM_PROMPT = \"Responda com a descrição do produto baseado no título fornecido.\"\n",
    "\n",
    "def prepare_data(input_path, output_path):\n",
    "    with open(input_path, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    formatted = []\n",
    "    for line in lines:\n",
    "        item = json.loads(line)\n",
    "        title = item.get(\"title\", \"\").strip()\n",
    "        content = item.get(\"content\", \"\").strip()\n",
    "        if not title or not content:\n",
    "            continue\n",
    "        prompt = {\n",
    "            \"instruction\": SYSTEM_PROMPT,\n",
    "            \"input\": item[\"title\"],\n",
    "            \"output\": content\n",
    "        }\n",
    "        formatted.append(json.dumps(prompt) + \"\\n\")\n",
    "\n",
    "    with open(output_path, \"w\") as f:\n",
    "        f.writelines(formatted)\n",
    "\n",
    "prepare_data(f\"{DATA_PATH}trn.json\", f\"{OUTPUT_PATH_DATASET}train_data.jsonl\")\n",
    "prepare_data(f\"{DATA_PATH}tst.json\", f\"{OUTPUT_PATH_DATASET}test_data.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b8e03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# 🧠 Carregar Modelo com Unsloth\n",
    "# ===============================\n",
    "import torch\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "# Carrega o modelo 4-bit\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"unsloth/tinyllama-bnb-4bit\",\n",
    "    max_seq_length=1024,  # Reduzido para acelerar\n",
    "    dtype=None,\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16,\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0.1,\n",
    "    bias = \"none\",\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],  # padrão do TinyLlama\n",
    ")\n",
    "\n",
    "# 🔧 Esse é o ponto onde preparamos o modelo para fine-tuning\n",
    "model = FastLanguageModel.for_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f2a81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# 🔍 Teste do Modelo ANTES do Fine-Tuning\n",
    "# ===============================\n",
    "def gerar_resposta_pretreino(produto):\n",
    "    prompt = f\"### Instrução:\\nDescreva o seguinte produto:\\n### Produto:\\n{produto}\\n### Resposta:\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    outputs = model.generate(**inputs, max_new_tokens=150)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# 🔬 Exemplo de teste antes do fine-tuning\n",
    "print(\"🔍 Resultado ANTES do treino:\")\n",
    "print(gerar_resposta_pretreino(\"Mog's Family of Cats\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6eecdb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# 📚 Carregar Dataset\n",
    "# ===============================\n",
    "from datasets import load_dataset\n",
    "\n",
    "path = f\"{OUTPUT_PATH_DATASET}train_data.jsonl\"\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files=path, split=\"train\")\n",
    "dataset = dataset.shuffle(seed=42).select(range(50000))  # Reduz para 50k exemplos (mais rápido)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631eecd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define os parâmetros de treinamento\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"model\",\n",
    "    logging_dir=\"logs\",\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=2,\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=2e-5,\n",
    "    logging_steps=50,\n",
    "    fp16=True,\n",
    "    report_to=\"none\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=1,\n",
    ")\n",
    "\n",
    "# Inicia o treinador com o modelo LoRA\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"output\",\n",
    "    max_seq_length = 1024,\n",
    "    tokenizer = tokenizer,\n",
    "    args = training_args,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# ✅ Remove os adapters para uso em inferência\n",
    "FastLanguageModel.for_inference(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74518976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# 💾 Salvar Modelo e Tokenizer para uso posterior\n",
    "# ===============================\n",
    "model.save_pretrained(MODEL_PATH_DATASET)\n",
    "tokenizer.save_pretrained(MODEL_PATH_DATASET)\n",
    "print(\"✅ Modelo salvo em:\", MODEL_PATH_DATASET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5121f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# 🧪 Função para Geração de Resposta\n",
    "# ===============================\n",
    "def gerar_resposta(produto):\n",
    "    prompt = f\"### Instrução:\\nDescreva o seguinte produto:\\n### Produto:\\n{produto}\\n### Resposta:\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    outputs = model.generate(**inputs, max_new_tokens=150)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# 🔍 Teste de exemplo\n",
    "print(gerar_resposta(\"Mog's Family of Cats\"))\n",
    "print(gerar_resposta(\"Why Don't They Just Quit? DVD Roundtable Discussion\"))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
